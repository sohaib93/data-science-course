{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification o MNIST dataset",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "PRnevs0Ugcj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "05d7457b-db35-42ab-e731-08c73ba8a34a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)\n",
        "#the mnist data has been split into training validation and test data sets. It has also been preprocessed"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J8pej3tzm3Qd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#outline model\n",
        "#we know we have 28x28 image so 28x28 = 784. Our input will be 784x1. Input units = 784\n",
        "#output units are 10 as we know the umebrs given are 1-10\n",
        "#we work with 2 hidden layers consisting of 50 units each\n",
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 600\n",
        "\n",
        "tf.reset_default_graph() #clears memory of all Variables left from previous tf runs\n",
        "\n",
        "inputs_placeholder = tf.placeholder(tf.float32,[None,input_size])\n",
        "targets_placeholder = tf.placeholder(tf.float32,[None,output_size])\n",
        "\n",
        "weights_1 = tf.get_variable(\"weigths_1\",[input_size,hidden_layer_size]) #if initialized this way, the default initializer is xavier\n",
        "biases_1 = tf.get_variable(\"biases_1\",[hidden_layer_size])\n",
        "\n",
        "#get output for 1st hidden layer .... using relu as the non linearity/activation function\n",
        "output_1 = tf.nn.relu(tf.matmul(inputs_placeholder,weights_1) + biases_1)\n",
        "\n",
        "#time to create second layer\n",
        "weigths_2 = tf.get_variable(\"weigths_2\",[hidden_layer_size,hidden_layer_size])\n",
        "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
        "output_2 = tf.nn.relu(tf.matmul(output_1,weigths_2) + biases_2)\n",
        "\n",
        "#####if you want a 3rd layer\n",
        "#3rd layer\n",
        "#weigths_3 = tf.get_variable(\"weigths_3\",[hidden_layer_size,hidden_layer_size])\n",
        "#biases_3 = tf.get_variable(\"biases_3\", [hidden_layer_size])\n",
        "#output_3 = tf.nn.relu(tf.matmul(output_2,weigths_3) + biases_3)\n",
        "#4th layer\n",
        "#weigths_4 = tf.get_variable(\"weigths_4\",[hidden_layer_size,hidden_layer_size])\n",
        "#biases_4 = tf.get_variable(\"biases_4\", [hidden_layer_size])\n",
        "#output_4 = tf.nn.relu(tf.matmul(output_3,weigths_4) + biases_4)\n",
        "#5th layer\n",
        "#weigths_5 = tf.get_variable(\"weigths_5\",[hidden_layer_size,hidden_layer_size])\n",
        "#biases_5 = tf.get_variable(\"biases_5\", [hidden_layer_size])\n",
        "#output_5 = tf.nn.relu(tf.matmul(output_4,weigths_5) + biases_5)\n",
        "\n",
        "\n",
        "#this is how we stack layers.\n",
        "#now moving on to the output layer\n",
        "\n",
        "weigths_11 = tf.get_variable(\"weigths_11\",[hidden_layer_size,output_size])\n",
        "biases_11 = tf.get_variable(\"biases_11\", [output_size])\n",
        "outputs = tf.matmul(output_2,weigths_11) + biases_11\n",
        "#will incorporate the activation function for outputs in the loss as it is common practice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Khp0-EJosm4t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.nn.softmax_cross_entropy_with_logits(logits = outputs, labels = targets_placeholder)\n",
        "#the above function applies a softmax activation and calculates a cross entropy loss\n",
        "#it is a numerically stable function. Obtaining very small number jeopardizes our model if we dont employ this\n",
        "#not sure if required here but better to be safe \n",
        "#logits here means unscaled probabilities. we want our output layer to produce probabilities\n",
        "#so the input into softmax is logit as the output of softmax gives probabilities\n",
        "\n",
        "#we have the loss but mean loss gives performance boost\n",
        "mean_loss = tf.reduce_mean(loss) #reduce_mean calculates the mean of a tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JlHZMJ9_-aUj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#now we choose the optimization method\n",
        "learning_rate = 0.00002\n",
        "optimize = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(mean_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eK3uY8h--d9x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#to test accuracy\n",
        "#as our output vector will give us probs. we want to get the highest prob of what it predicted and match \n",
        "#it to the actual target. if the indices of max of target and max of predicted is same, that means e predicted correct\n",
        "#tf.argmax(outputs,1) gets the highest of each column(axis 1) in putputs vector\n",
        "out_equals_target = tf.equal(tf.argmax(outputs,1),tf.argmax(targets_placeholder,1))\n",
        "#out_equals_target is a vector containing 1 if accurately predcted else 0. the accuracy is the mean of this vector\n",
        "accuracy = tf.reduce_mean(tf.cast(out_equals_target,tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X3HPFPra-gVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "44aaed22-6d55-4846-8511-02f591c5b58f"
      },
      "cell_type": "code",
      "source": [
        "# time to LEARN\n",
        "#first we initialize variabless\n",
        "session = tf.InteractiveSession()\n",
        "initializer = tf.global_variables_initializer()\n",
        "session.run(initializer)\n",
        "#then we do batching\n",
        "batch_size = 100\n",
        "batches_number = mnist.train._num_examples // batch_size #// means divide and give integer\n",
        "max_epochs = 50\n",
        "\n",
        "prev_validation_loss = 999999999. #giving it a high value just like max in java\n",
        "\n",
        "for epoch_counter in range(max_epochs):\n",
        "  curr_epoch_loss = 0\n",
        "  for batch_counter in range(batches_number):\n",
        "    input_batch,target_batch = mnist.train.next_batch(batch_size)\n",
        "    _, batch_loss = session.run([optimize,mean_loss], \n",
        "                                feed_dict={inputs_placeholder:input_batch, targets_placeholder: target_batch})\n",
        "    \n",
        "    curr_epoch_loss +=batch_loss\n",
        "    \n",
        "  curr_epoch_loss /= batches_number #this is the training loss\n",
        "  #now we will validate by forward propagating the validation set\n",
        "  input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
        "  validation_loss, validation_accuracy = session.run([mean_loss,accuracy],\n",
        "                                                     feed_dict={inputs_placeholder:input_batch, targets_placeholder: target_batch})\n",
        "  print('Epoch ' + str(epoch_counter) + '. Training loss: ' + '{0:3f}'.format(curr_epoch_loss)\n",
        "       + '. Validation loss: ' + '{0:3f}'.format(validation_loss)\n",
        "       + '. Validation Accuracy: ' + '{0:2f}'.format(validation_accuracy *100) + '%')\n",
        "  \n",
        "  #eary stop rule\n",
        "  if validation_loss > prev_validation_loss:\n",
        "    break\n",
        "    \n",
        "  prev_validation_loss = validation_loss\n",
        "  \n",
        "print ('End of Training')"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0. Training loss: 1.122101. Validation loss: 0.475835. Validation Accuracy: 88.880002%\n",
            "Epoch 1. Training loss: 0.392936. Validation loss: 0.313681. Validation Accuracy: 91.659999%\n",
            "Epoch 2. Training loss: 0.299052. Validation loss: 0.260788. Validation Accuracy: 93.080002%\n",
            "Epoch 3. Training loss: 0.255414. Validation loss: 0.230134. Validation Accuracy: 93.839997%\n",
            "Epoch 4. Training loss: 0.226506. Validation loss: 0.205857. Validation Accuracy: 94.340003%\n",
            "Epoch 5. Training loss: 0.204360. Validation loss: 0.189669. Validation Accuracy: 94.819999%\n",
            "Epoch 6. Training loss: 0.187024. Validation loss: 0.175249. Validation Accuracy: 95.120001%\n",
            "Epoch 7. Training loss: 0.172230. Validation loss: 0.163257. Validation Accuracy: 95.380002%\n",
            "Epoch 8. Training loss: 0.159559. Validation loss: 0.153420. Validation Accuracy: 95.639998%\n",
            "Epoch 9. Training loss: 0.148523. Validation loss: 0.147079. Validation Accuracy: 96.120000%\n",
            "Epoch 10. Training loss: 0.139003. Validation loss: 0.138761. Validation Accuracy: 96.300000%\n",
            "Epoch 11. Training loss: 0.130225. Validation loss: 0.130827. Validation Accuracy: 96.420002%\n",
            "Epoch 12. Training loss: 0.122687. Validation loss: 0.125256. Validation Accuracy: 96.679997%\n",
            "Epoch 13. Training loss: 0.115735. Validation loss: 0.120620. Validation Accuracy: 96.560001%\n",
            "Epoch 14. Training loss: 0.109462. Validation loss: 0.116397. Validation Accuracy: 96.820003%\n",
            "Epoch 15. Training loss: 0.103449. Validation loss: 0.112055. Validation Accuracy: 96.840000%\n",
            "Epoch 16. Training loss: 0.098127. Validation loss: 0.108573. Validation Accuracy: 97.000003%\n",
            "Epoch 17. Training loss: 0.093125. Validation loss: 0.104316. Validation Accuracy: 97.160000%\n",
            "Epoch 18. Training loss: 0.088549. Validation loss: 0.100919. Validation Accuracy: 97.180003%\n",
            "Epoch 19. Training loss: 0.084232. Validation loss: 0.098370. Validation Accuracy: 97.219998%\n",
            "Epoch 20. Training loss: 0.080269. Validation loss: 0.097798. Validation Accuracy: 97.240001%\n",
            "Epoch 21. Training loss: 0.076454. Validation loss: 0.093898. Validation Accuracy: 97.259998%\n",
            "Epoch 22. Training loss: 0.073066. Validation loss: 0.091031. Validation Accuracy: 97.320002%\n",
            "Epoch 23. Training loss: 0.069461. Validation loss: 0.089008. Validation Accuracy: 97.420001%\n",
            "Epoch 24. Training loss: 0.066385. Validation loss: 0.087347. Validation Accuracy: 97.439998%\n",
            "Epoch 25. Training loss: 0.063436. Validation loss: 0.085669. Validation Accuracy: 97.420001%\n",
            "Epoch 26. Training loss: 0.060530. Validation loss: 0.085614. Validation Accuracy: 97.600001%\n",
            "Epoch 27. Training loss: 0.058079. Validation loss: 0.082288. Validation Accuracy: 97.539997%\n",
            "Epoch 28. Training loss: 0.055377. Validation loss: 0.080529. Validation Accuracy: 97.600001%\n",
            "Epoch 29. Training loss: 0.053030. Validation loss: 0.080170. Validation Accuracy: 97.600001%\n",
            "Epoch 30. Training loss: 0.050725. Validation loss: 0.078699. Validation Accuracy: 97.680002%\n",
            "Epoch 31. Training loss: 0.048418. Validation loss: 0.077996. Validation Accuracy: 97.740000%\n",
            "Epoch 32. Training loss: 0.046282. Validation loss: 0.075795. Validation Accuracy: 97.839999%\n",
            "Epoch 33. Training loss: 0.044367. Validation loss: 0.076311. Validation Accuracy: 97.799999%\n",
            "End of Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VAPWtbExCXuH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01f83603-1c5d-496f-8ae2-be276d2d06ce"
      },
      "cell_type": "code",
      "source": [
        "#time to test using test data set\n",
        "\n",
        "input_batch,target_batch = mnist.test.next_batch(mnist.test._num_examples) #get input and outputs of test data\n",
        "test_accuracy = session.run([accuracy], feed_dict = {inputs_placeholder:input_batch, targets_placeholder: target_batch})\n",
        "print('{0:3f}'.format(test_accuracy[0]*100) + '%')"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97.710001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tJ1YTHJuEyak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#final accuracy is as shown above"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zjVkhAxiFtId",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initial Hyperparams:**\n",
        "\n",
        "input_size = 784, \n",
        "output_size = 10, \n",
        "hidden_layer_size = 50, \n",
        "learning_rate = 0.001, \n",
        "batch_size = 100\n",
        "\n",
        "TIME TAKEN: 15s\n",
        "\n",
        "ACCURACY 97.10%\n",
        "\n",
        "\n",
        "\n",
        "**1. The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?**\n",
        "\n",
        "TIME TAKEN: 15s\n",
        "\n",
        "ACCURACY 97.77%\n",
        "\n",
        "The validation accuracy is significantly higher (as the algorithm with 50 hidden units was too simple of a model).\n",
        "\n",
        "Naturally, it takes the algorithm much longer to train (unless early stopping is triggered too soon).\n",
        "\n",
        "A hidden layer size of 500 (and not only) works better.\n",
        "\n",
        "\n",
        "**2. The *depth* of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases.**\n",
        "\n",
        "With 4 layers, time taken was 12s and accuracy was 97.03%\n",
        "\n",
        "\n",
        "**3. The *width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?**\n",
        "\n",
        "With 4 layers, time taken was 10s and accuracy was 96.16%\n",
        "\n",
        "With 4 layers, time taken was 10s and accuracy was 96.44%\n",
        "\n",
        "**4. Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the method: tf.nn.sigmoid()**\n",
        "\n",
        "Time taken: 19 s\n",
        "\n",
        "Accuracy: 96.8%. on average took more epochs to train.\n",
        "\n",
        "Generally, we should reach an inferior solution. That is because relu 'cleans' the noise in the data\n",
        "\n",
        "**5. Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the method: tf.nn.tanh()**\n",
        "\n",
        "TIME TAKEN: 13s\n",
        "\n",
        "ACCURACY 97.15%\n",
        "\n",
        "**6. Adjust the batch size. Try a batch size of 1000. How does the required time change? What about the accuracy?**\n",
        "\n",
        "TIME TAKEN: 13s\n",
        "\n",
        "ACCURACY 96%. Did not trigger early stop in the 15 epochs. The accuracy in the limited  epochs was lower than in batch size 100. A bigger batch size results in slower training\n",
        "\n",
        "**7. Adjust the batch size. Try a batch size of 1. That's the SGD. How do the time and accuracy change? Is the result coherent with the theory?**\n",
        "\n",
        "TIME TAKEN: 125s\n",
        "\n",
        "ACCURACY 96%.  triggered early stop in the the 2nd epoch. Took a minute approx each for each epoch. the accuracy was compritively lower too\n",
        "\n",
        "**8. Adjust the learning rate. Try a value of 0.0001. Does it make a difference?**\n",
        "\n",
        "TIME TAKEN: 21s\n",
        "\n",
        "ACCURACY 95.4%. The learning rate was too small and hence it gradually the accuracy was improving but in the 15 epochs it still did not come close to as accurate our previous models have been. Since the training is lower, I have also changed the max_epochs to 50, otherwise early stopping would be triggered.\n",
        "\n",
        "The result is basically the same, but we reach it much slower.\n",
        "\n",
        "**9. Adjust the learning rate. Try a value of 0.02. Does it make a difference?**\n",
        "\n",
        "TIME TAKEN: 7s\n",
        "\n",
        "ACCURACY 94%. The learning rate was high and accuracy was lower\n",
        "\n",
        "**The final version i have is by playing around with the hyper params. Not necessarily the best accuracy wise, but was good for practice**\n",
        "\n",
        "input_size = 784, \n",
        "output_size = 10, \n",
        "hidden_layer_size = 600, \n",
        "learning_rate =0.00002, \n",
        "batch_size = 100\n",
        "\n",
        "ACCURACY 97.71%\n"
      ]
    },
    {
      "metadata": {
        "id": "g0XTSAf1Fyj3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}