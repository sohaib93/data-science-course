{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML algo using TensorFlow",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "QMX6phpMSptv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## ML Algo with TensorFlow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FuJRhls_Sue1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sPYuEndmS8OP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "no_of_observations = 1000\n",
        "\n",
        "x1 = np.random.uniform(-10,10,size = (no_of_observations,1)) #the size of this vector x1 should be (no of obs x 1)\n",
        "x2 = np.random.uniform(-10,10,size = (no_of_observations,1))\n",
        "inputs = np.column_stack((x1,x2))\n",
        "\n",
        "noise = np.random.uniform(-1, 1, (no_of_observations,1))\n",
        "targets = 2*x1 - 3*x2 + 5 + noise\n",
        "\n",
        "np.savez('training.npz', inputs = inputs, targets = targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZLentrGTYKI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Solving now with TensorFlow\n",
        "input_size = 2 # since there are 2 independetn vars x1 and x2\n",
        "output_size = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MjHD6-4IUFSr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# inputs that need to go into tensor flow will go into placeholders. weights and biases go into variables\n",
        "\n",
        "inputs_placeholder = tf.placeholder(tf.float32, [None,input_size]) # params are datatype and dimensions (None x input size). None here means that nothing specified so the no of rows can by anything\n",
        "targets_placeholder = tf.placeholder(tf.float32, [None,output_size])\n",
        "\n",
        "weights = tf.Variable(tf.random_uniform([input_size,output_size], minval = -0.1, maxval=0.1))\n",
        "biases = tf.Variable(tf.random_uniform([output_size], minval = -0.1, maxval=0.1))\n",
        "\n",
        "#now write the model equation y = wx + b\n",
        "outputs = tf.matmul(inputs_placeholder,weights) +biases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MdCEmHZjYWOY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#now we need an objective function\n",
        "#loss here would be l2 norm so, loss = (sum of (xi -ti)^2)/no of obs / 2, which in other words is pretty much the error in our outcome from targets per observation\n",
        "#inside tf, there is a module losses which contains maths/funcionality of the most common loss functions\n",
        "mean_loss = tf.losses.mean_squared_error(labels = targets_placeholder, predictions = outputs) /2 #2. so that answers are float and not int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XpOrBjC8ZdcW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#now we need an optimization algo\n",
        "#inside tf, there is a module losses train contains maths/funcionality of the most common optimization algos\n",
        "learning_rate = 0.05\n",
        "optimize = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(mean_loss) #we use gradient descent. We specified learning rate to 0.05 and we want to minimize the mean loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sv0xrWo1aiyD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "39e2f388-c35d-44f9-feac-db5bfdf77605"
      },
      "cell_type": "code",
      "source": [
        "#nothing has been executed yet but we have created everything required to learn\n",
        "#in tf, the execution takes place in sessions. When we say tf.InteractiveSession. this is the tf classs we use wehnever we want to execute anything in tf world\n",
        "#training in tf happens in these sessions and wehenever we use interactive session we mean to execute the process \n",
        "#storing this in a variable so i dont have to type the whole thing again everytime\n",
        "\n",
        "session = tf.InteractiveSession()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "yA-pQwmKBJeu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#time to initialize variables\n",
        "#weights and biases need to have some init values\n",
        "\n",
        "initializer = tf.global_variables_initializer() #this method initializes all tensor objects marked as 'variables'\n",
        "session.run(initializer) #execution of each step, we need to use this run method"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tAcZe1HLC8hF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f62a16a4-77da-48de-8e7c-9400783e28b8"
      },
      "cell_type": "code",
      "source": [
        "training_data = np.load('training.npz')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "Lu9hWlvaGwEN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#now we needa  for loop for learning\n",
        "\n",
        "for epoch in range(100):\n",
        "  _, curr_loss = session.run([optimize,mean_loss],feed_dict={inputs_placeholder:training_data['inputs'], targets_placeholder :training_data['targets']}) \n",
        "  #feed dict tells the algo how the data will be fed\n",
        "  # np.savez('training.npz', inputs = inputs, targets = targets) this is wehre we got the labels of the data\n",
        "  #the above call means that run the optimize and mean loss operations by feeding data specified in the placeholders\n",
        "  \n",
        "  print(curr_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3lnjfA1ELggV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "_YMH7AHRLhBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "c1af319f-8fa9-4b30-8920-3049f5bc20c8"
      },
      "cell_type": "code",
      "source": [
        "# Trainign is over as our cut off was 100 as in loop. now we have trained weights and biases adn hence now we just need the ouputs that we will run below\n",
        "# Same notation as above but this time we don't want to train anymore, and we are not interested\n",
        "# in the loss function value.\n",
        "out = session.run([outputs], \n",
        "               feed_dict={inputs_placeholder: training_data['inputs']})\n",
        "# The model is optimized, so the outputs are calculated based on the last form of the model\n",
        "\n",
        "# We have to np.squeeze the arrays in order to fit them to what the plot function expects.\n",
        "# Doesn't change anything as we cut dimensions of size 1 - just a technicality.\n",
        "plt.plot(np.squeeze(out), np.squeeze(training_data['targets']))\n",
        "plt.xlabel('outputs')\n",
        "plt.ylabel('targets')\n",
        "plt.show()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFYCAYAAACoFn5YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXwgZ7PK7+UNIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fcc072e4f60>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "m6TtSKSgd87v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Some takeaways if we had used huber_loss as compared to mean squared loss:\n",
        "\n",
        "#Any function that has the property to be lower for better results and higher for worse results can be a loss function. This includes the Huber loss.\n",
        "#Almost everything seems identical.\n",
        "#The values of the loss are generally lower (because of the Huber loss formula, and the convexity of the two functions).\n",
        "#For our problem, both the L2-norm loss and the Huber loss work equally well.\n",
        "#Generally, the Huber loss is used when we have a lot of outliers."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}