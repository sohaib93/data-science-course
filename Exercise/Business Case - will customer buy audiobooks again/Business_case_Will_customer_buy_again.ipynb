{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Business case_Will customer buy again.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "vK-EXG3wStPV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This class has been copied from the tutorial in course\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1LAARIwCS6m8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a class that will do the batching for the algorithm\n",
        "# This code is extremely reusable. You should just change Audiobooks_data everywhere in the code\n",
        "class Data_Reader():\n",
        "    # Dataset is a mandatory arugment, while the batch_size is optional\n",
        "    # If you don't input batch_size, it will automatically take the value: None\n",
        "    def __init__(self, dataset, batch_size = None):\n",
        "    \n",
        "        # The dataset that loads is one of \"train\", \"validation\", \"test\".\n",
        "        # e.g. if I call this class with x('train',5), it will load 'Audiobooks_data_train.npz' with a batch size of 5.\n",
        "        npz = np.load('Audiobook_{0}_Data.npz'.format(dataset))\n",
        "        \n",
        "        # Two variables that take the values of the inputs and the targets. Inputs are floats, targets are integers\n",
        "        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
        "        \n",
        "        # Counts the batch number, given the size you feed it later\n",
        "        # If the batch size is None, we are either validating or testing, so we want to take the data in a single batch\n",
        "        if batch_size is None:\n",
        "            self.batch_size = self.inputs.shape[0]\n",
        "        else:\n",
        "            self.batch_size = batch_size\n",
        "        self.curr_batch = 0\n",
        "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
        "    \n",
        "    # A method which loads the next batch\n",
        "    def __next__(self):\n",
        "        if self.curr_batch >= self.batch_count:\n",
        "            self.curr_batch = 0\n",
        "            raise StopIteration()\n",
        "            \n",
        "        # You slice the dataset in batches and then the \"next\" function loads them one after the other\n",
        "        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
        "        inputs_batch = self.inputs[batch_slice]\n",
        "        targets_batch = self.targets[batch_slice]\n",
        "        self.curr_batch += 1\n",
        "        \n",
        "        # One-hot encode the targets. In this example it's a bit superfluous since we have a 0/1 column \n",
        "        # as a target already but we're giving you the code regardless, as it will be useful for any \n",
        "        # classification task with more than one target column\n",
        "        classes_num = 2\n",
        "        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n",
        "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
        "        \n",
        "        # The function will return the inputs batch and the one-hot encoded targets\n",
        "        return inputs_batch, targets_one_hot\n",
        "    \n",
        "        \n",
        "    # A method needed for iterating over the batches, as we will put them in a loop\n",
        "    # This tells Python that the class we're defining is iterable, i.e. that we can use it like:\n",
        "    # for input, output in data: \n",
        "        # do things\n",
        "    # An iterator in Python is a class with a method __next__ that defines exactly how to iterate through its objects\n",
        "    def __iter__(self):\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kRfoQmj-We7M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######outline model\n",
        "#10 columns in data set so 10 inputs, 2 possible targets... we will use 2 hidden layers initially with units 50 each\n",
        "\n",
        "input_size = 10\n",
        "output_size = 2\n",
        "hidden_layer_size = 50\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "inputs_placeholder = tf.placeholder(tf.float32,[None,input_size])\n",
        "targets_placeholder = tf.placeholder(tf.int32,[None,output_size])\n",
        "\n",
        "\n",
        "#hidden layer 1\n",
        "weights_1 = tf.get_variable(\"weigths_1\",[input_size,hidden_layer_size]) #if initialized this way, the default initializer is xavier\n",
        "biases_1 = tf.get_variable(\"biases_1\",[hidden_layer_size])\n",
        "output_1 = tf.nn.sigmoid(tf.matmul(inputs_placeholder,weights_1) + biases_1)\n",
        "\n",
        "#hidden layer 2\n",
        "weights_2 = tf.get_variable(\"weigths_2\",[hidden_layer_size,hidden_layer_size]) #if initialized this way, the default initializer is xavier\n",
        "biases_2 = tf.get_variable(\"biases_2\",[hidden_layer_size])\n",
        "output_2 = tf.nn.sigmoid(tf.matmul(output_1,weights_2) + biases_2)\n",
        "\n",
        "#hidden layer 3\n",
        "weights_3 = tf.get_variable(\"weigths_3\",[hidden_layer_size,hidden_layer_size]) #if initialized this way, the default initializer is xavier\n",
        "biases_3 = tf.get_variable(\"biases_3\",[hidden_layer_size])\n",
        "output_3 = tf.nn.sigmoid(tf.matmul(output_2,weights_3) + biases_3)\n",
        "\n",
        "#output layer\n",
        "weights_4 = tf.get_variable(\"weigths_4\",[hidden_layer_size,output_size]) #if initialized this way, the default initializer is xavier\n",
        "biases_4 = tf.get_variable(\"biases_4\",[output_size])\n",
        "outputs = tf.matmul(output_3,weights_4) + biases_4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y_Xztfw1-FUD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#calculate loss/delta\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs,labels=targets_placeholder)\n",
        "\n",
        "mean_loss = tf.reduce_mean(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QtEIUDfX2v5Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#optimize\n",
        "learning_rate = 0.001\n",
        "optimize = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(mean_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tFg-PZTD3mDf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test accuracy\n",
        "out_equals_target = tf.equal(tf.argmax(outputs,1),tf.argmax(targets_placeholder,1))\n",
        "#out_equals_target is a vector containing 1 if accurately predcted else 0. the accuracy is the mean of this vector\n",
        "accuracy = tf.reduce_mean(tf.cast(out_equals_target,tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSs8FubX36QX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c7d7ed62-911c-4699-b6e1-ffc8bdc23e48"
      },
      "cell_type": "code",
      "source": [
        "#create session var\n",
        "session = tf.InteractiveSession()\n",
        "#initialize variables\n",
        "initializer = tf.global_variables_initializer()\n",
        "session.run(initializer)"
      ],
      "execution_count": 437,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "f6az5tiV4UAl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#batching\n",
        "batch_size = 50\n",
        "#batch number is handled in Data_Reader class\n",
        "max_epoch = 100\n",
        "prev_validation_loss = 9999999999.\n",
        "\n",
        "#get data\n",
        "training_data = Data_Reader('Training',batch_size)\n",
        "validation_data = Data_Reader('Validation',batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uCq8RTOe6ArR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        },
        "outputId": "f556289d-a249-4cf6-e32c-8d14e51eb23a"
      },
      "cell_type": "code",
      "source": [
        "#train model\n",
        "for epoch in range(max_epoch):\n",
        "  curr_epoch_loss = 0\n",
        "  #train\n",
        "  for input_batch, target_batch in training_data:\n",
        "    _, batch_loss = session.run([optimize,mean_loss], feed_dict = {inputs_placeholder: input_batch, targets_placeholder:target_batch})\n",
        "    curr_epoch_loss += batch_loss\n",
        "  \n",
        "  curr_epoch_loss /= training_data.batch_count\n",
        "  \n",
        "  #validate\n",
        "  validation_loss = 0.\n",
        "  validation_accuracy = 0.\n",
        "  for input_batch, target_batch in validation_data: #this will always have a single iteration but easy way to get input and target batches\n",
        "    validation_loss, validation_accuracy = session.run([mean_loss,accuracy], feed_dict = {inputs_placeholder: input_batch, targets_placeholder:target_batch})\n",
        "  \n",
        "  \n",
        "  print('Epoch '+str(epoch+1)\n",
        "         + '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss)\n",
        "         +'. Validation loss: '+'{0:.3f}'.format(validation_loss)\n",
        "         +'. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
        "  #early stop\n",
        "  if validation_loss > prev_validation_loss:\n",
        "    break\n",
        "    \n",
        "  prev_validation_loss = validation_loss\n",
        "  \n",
        "print('End of Training')"
      ],
      "execution_count": 439,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1. Training loss: 0.716. Validation loss: 0.687. Validation accuracy: 48.00%\n",
            "Epoch 2. Training loss: 0.675. Validation loss: 0.660. Validation accuracy: 58.00%\n",
            "Epoch 3. Training loss: 0.611. Validation loss: 0.568. Validation accuracy: 66.00%\n",
            "Epoch 4. Training loss: 0.500. Validation loss: 0.461. Validation accuracy: 72.00%\n",
            "Epoch 5. Training loss: 0.433. Validation loss: 0.404. Validation accuracy: 78.00%\n",
            "Epoch 6. Training loss: 0.406. Validation loss: 0.381. Validation accuracy: 76.00%\n",
            "Epoch 7. Training loss: 0.391. Validation loss: 0.371. Validation accuracy: 80.00%\n",
            "Epoch 8. Training loss: 0.382. Validation loss: 0.365. Validation accuracy: 78.00%\n",
            "Epoch 9. Training loss: 0.376. Validation loss: 0.361. Validation accuracy: 82.00%\n",
            "Epoch 10. Training loss: 0.372. Validation loss: 0.358. Validation accuracy: 82.00%\n",
            "Epoch 11. Training loss: 0.368. Validation loss: 0.357. Validation accuracy: 82.00%\n",
            "Epoch 12. Training loss: 0.365. Validation loss: 0.355. Validation accuracy: 76.00%\n",
            "Epoch 13. Training loss: 0.363. Validation loss: 0.354. Validation accuracy: 76.00%\n",
            "Epoch 14. Training loss: 0.361. Validation loss: 0.353. Validation accuracy: 76.00%\n",
            "Epoch 15. Training loss: 0.359. Validation loss: 0.352. Validation accuracy: 76.00%\n",
            "Epoch 16. Training loss: 0.358. Validation loss: 0.351. Validation accuracy: 76.00%\n",
            "Epoch 17. Training loss: 0.356. Validation loss: 0.351. Validation accuracy: 76.00%\n",
            "Epoch 18. Training loss: 0.355. Validation loss: 0.350. Validation accuracy: 76.00%\n",
            "Epoch 19. Training loss: 0.354. Validation loss: 0.350. Validation accuracy: 74.00%\n",
            "Epoch 20. Training loss: 0.353. Validation loss: 0.349. Validation accuracy: 74.00%\n",
            "Epoch 21. Training loss: 0.352. Validation loss: 0.349. Validation accuracy: 76.00%\n",
            "Epoch 22. Training loss: 0.351. Validation loss: 0.348. Validation accuracy: 76.00%\n",
            "Epoch 23. Training loss: 0.351. Validation loss: 0.348. Validation accuracy: 76.00%\n",
            "Epoch 24. Training loss: 0.350. Validation loss: 0.348. Validation accuracy: 76.00%\n",
            "Epoch 25. Training loss: 0.349. Validation loss: 0.347. Validation accuracy: 76.00%\n",
            "Epoch 26. Training loss: 0.349. Validation loss: 0.347. Validation accuracy: 76.00%\n",
            "Epoch 27. Training loss: 0.348. Validation loss: 0.347. Validation accuracy: 76.00%\n",
            "Epoch 28. Training loss: 0.347. Validation loss: 0.347. Validation accuracy: 76.00%\n",
            "Epoch 29. Training loss: 0.347. Validation loss: 0.346. Validation accuracy: 76.00%\n",
            "Epoch 30. Training loss: 0.346. Validation loss: 0.346. Validation accuracy: 76.00%\n",
            "Epoch 31. Training loss: 0.346. Validation loss: 0.346. Validation accuracy: 76.00%\n",
            "Epoch 32. Training loss: 0.345. Validation loss: 0.346. Validation accuracy: 76.00%\n",
            "Epoch 33. Training loss: 0.345. Validation loss: 0.345. Validation accuracy: 76.00%\n",
            "Epoch 34. Training loss: 0.344. Validation loss: 0.345. Validation accuracy: 76.00%\n",
            "Epoch 35. Training loss: 0.344. Validation loss: 0.345. Validation accuracy: 76.00%\n",
            "Epoch 36. Training loss: 0.343. Validation loss: 0.345. Validation accuracy: 76.00%\n",
            "Epoch 37. Training loss: 0.343. Validation loss: 0.344. Validation accuracy: 76.00%\n",
            "Epoch 38. Training loss: 0.342. Validation loss: 0.344. Validation accuracy: 78.00%\n",
            "Epoch 39. Training loss: 0.342. Validation loss: 0.344. Validation accuracy: 78.00%\n",
            "Epoch 40. Training loss: 0.341. Validation loss: 0.344. Validation accuracy: 78.00%\n",
            "Epoch 41. Training loss: 0.341. Validation loss: 0.344. Validation accuracy: 78.00%\n",
            "Epoch 42. Training loss: 0.340. Validation loss: 0.343. Validation accuracy: 78.00%\n",
            "Epoch 43. Training loss: 0.340. Validation loss: 0.343. Validation accuracy: 78.00%\n",
            "Epoch 44. Training loss: 0.339. Validation loss: 0.343. Validation accuracy: 78.00%\n",
            "Epoch 45. Training loss: 0.339. Validation loss: 0.343. Validation accuracy: 78.00%\n",
            "Epoch 46. Training loss: 0.339. Validation loss: 0.342. Validation accuracy: 78.00%\n",
            "Epoch 47. Training loss: 0.338. Validation loss: 0.342. Validation accuracy: 78.00%\n",
            "Epoch 48. Training loss: 0.338. Validation loss: 0.342. Validation accuracy: 78.00%\n",
            "Epoch 49. Training loss: 0.337. Validation loss: 0.342. Validation accuracy: 78.00%\n",
            "Epoch 50. Training loss: 0.337. Validation loss: 0.341. Validation accuracy: 78.00%\n",
            "Epoch 51. Training loss: 0.336. Validation loss: 0.341. Validation accuracy: 78.00%\n",
            "Epoch 52. Training loss: 0.336. Validation loss: 0.341. Validation accuracy: 78.00%\n",
            "Epoch 53. Training loss: 0.336. Validation loss: 0.341. Validation accuracy: 78.00%\n",
            "Epoch 54. Training loss: 0.335. Validation loss: 0.340. Validation accuracy: 78.00%\n",
            "Epoch 55. Training loss: 0.335. Validation loss: 0.340. Validation accuracy: 78.00%\n",
            "Epoch 56. Training loss: 0.334. Validation loss: 0.340. Validation accuracy: 78.00%\n",
            "Epoch 57. Training loss: 0.334. Validation loss: 0.340. Validation accuracy: 78.00%\n",
            "Epoch 58. Training loss: 0.334. Validation loss: 0.340. Validation accuracy: 78.00%\n",
            "Epoch 59. Training loss: 0.333. Validation loss: 0.339. Validation accuracy: 78.00%\n",
            "Epoch 60. Training loss: 0.333. Validation loss: 0.339. Validation accuracy: 84.00%\n",
            "Epoch 61. Training loss: 0.332. Validation loss: 0.339. Validation accuracy: 84.00%\n",
            "Epoch 62. Training loss: 0.332. Validation loss: 0.339. Validation accuracy: 84.00%\n",
            "Epoch 63. Training loss: 0.332. Validation loss: 0.338. Validation accuracy: 84.00%\n",
            "Epoch 64. Training loss: 0.331. Validation loss: 0.338. Validation accuracy: 84.00%\n",
            "Epoch 65. Training loss: 0.331. Validation loss: 0.338. Validation accuracy: 84.00%\n",
            "Epoch 66. Training loss: 0.331. Validation loss: 0.338. Validation accuracy: 84.00%\n",
            "Epoch 67. Training loss: 0.330. Validation loss: 0.337. Validation accuracy: 84.00%\n",
            "Epoch 68. Training loss: 0.330. Validation loss: 0.337. Validation accuracy: 84.00%\n",
            "Epoch 69. Training loss: 0.330. Validation loss: 0.337. Validation accuracy: 84.00%\n",
            "Epoch 70. Training loss: 0.329. Validation loss: 0.337. Validation accuracy: 84.00%\n",
            "Epoch 71. Training loss: 0.329. Validation loss: 0.337. Validation accuracy: 84.00%\n",
            "Epoch 72. Training loss: 0.329. Validation loss: 0.336. Validation accuracy: 84.00%\n",
            "Epoch 73. Training loss: 0.329. Validation loss: 0.336. Validation accuracy: 84.00%\n",
            "Epoch 74. Training loss: 0.328. Validation loss: 0.336. Validation accuracy: 84.00%\n",
            "Epoch 75. Training loss: 0.328. Validation loss: 0.336. Validation accuracy: 84.00%\n",
            "Epoch 76. Training loss: 0.328. Validation loss: 0.336. Validation accuracy: 84.00%\n",
            "Epoch 77. Training loss: 0.328. Validation loss: 0.335. Validation accuracy: 84.00%\n",
            "Epoch 78. Training loss: 0.327. Validation loss: 0.335. Validation accuracy: 84.00%\n",
            "Epoch 79. Training loss: 0.327. Validation loss: 0.335. Validation accuracy: 84.00%\n",
            "Epoch 80. Training loss: 0.327. Validation loss: 0.335. Validation accuracy: 84.00%\n",
            "Epoch 81. Training loss: 0.327. Validation loss: 0.335. Validation accuracy: 84.00%\n",
            "Epoch 82. Training loss: 0.326. Validation loss: 0.335. Validation accuracy: 84.00%\n",
            "Epoch 83. Training loss: 0.326. Validation loss: 0.334. Validation accuracy: 84.00%\n",
            "Epoch 84. Training loss: 0.326. Validation loss: 0.334. Validation accuracy: 84.00%\n",
            "Epoch 85. Training loss: 0.326. Validation loss: 0.334. Validation accuracy: 84.00%\n",
            "Epoch 86. Training loss: 0.326. Validation loss: 0.334. Validation accuracy: 84.00%\n",
            "Epoch 87. Training loss: 0.325. Validation loss: 0.334. Validation accuracy: 84.00%\n",
            "Epoch 88. Training loss: 0.325. Validation loss: 0.334. Validation accuracy: 84.00%\n",
            "Epoch 89. Training loss: 0.325. Validation loss: 0.334. Validation accuracy: 84.00%\n",
            "Epoch 90. Training loss: 0.325. Validation loss: 0.334. Validation accuracy: 84.00%\n",
            "Epoch 91. Training loss: 0.325. Validation loss: 0.333. Validation accuracy: 84.00%\n",
            "Epoch 92. Training loss: 0.325. Validation loss: 0.333. Validation accuracy: 84.00%\n",
            "Epoch 93. Training loss: 0.324. Validation loss: 0.333. Validation accuracy: 84.00%\n",
            "Epoch 94. Training loss: 0.324. Validation loss: 0.333. Validation accuracy: 84.00%\n",
            "Epoch 95. Training loss: 0.324. Validation loss: 0.333. Validation accuracy: 84.00%\n",
            "Epoch 96. Training loss: 0.324. Validation loss: 0.333. Validation accuracy: 84.00%\n",
            "Epoch 97. Training loss: 0.324. Validation loss: 0.333. Validation accuracy: 84.00%\n",
            "Epoch 98. Training loss: 0.324. Validation loss: 0.333. Validation accuracy: 84.00%\n",
            "Epoch 99. Training loss: 0.324. Validation loss: 0.333. Validation accuracy: 84.00%\n",
            "Epoch 100. Training loss: 0.323. Validation loss: 0.333. Validation accuracy: 84.00%\n",
            "End of Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OeTGLD-J_Iou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56ebe230-e227-4e71-8264-aa8d04bf9d2f"
      },
      "cell_type": "code",
      "source": [
        "#test the model\n",
        "\n",
        "test_data = Data_Reader('Test',batch_size)\n",
        "for input_batch, target_batch in test_data: \n",
        "  test_accuracy = session.run([accuracy], feed_dict = {inputs_placeholder: input_batch, targets_placeholder:target_batch})\n",
        "    \n",
        "print('{0:3f}'.format(test_accuracy[0]*100) + '%')"
      ],
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86.000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C-_1KaNBLTMs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#after tuning and playing around with the hyperparams:\n",
        "#Accuracy = 86%\n",
        "\n",
        "#Hyperparams:\n",
        "#batchsize = 50\n",
        "#hidden layer = 3\n",
        "#hidden_layer_size = 50\n",
        "#learning rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}